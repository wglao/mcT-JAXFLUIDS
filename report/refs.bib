@article{bezgin22,
	title = {{JAX}-{FLUIDS}: {A} fully-differentiable high-order computational fluid dynamics solver for compressible two-phase flows},
	volume = {282},
	issn = {00104655},
	shorttitle = {{JAX}-{FLUIDS}},
	url = {http://arxiv.org/abs/2203.13760},
	doi = {10.1016/j.cpc.2022.108527},
	abstract = {Physical systems are governed by partial differential equations (PDEs). The Navier-Stokes equations describe fluid flows and are representative of nonlinear physical systems with complex spatio-temporal interactions. Fluid flows are omnipresent in nature and engineering applications, and their accurate simulation is essential for providing insights into these processes. While PDEs are typically solved with numerical methods, the recent success of machine learning (ML) has shown that ML methods can provide novel avenues of finding solutions to PDEs. ML is becoming more and more present in computational fluid dynamics (CFD). However, up to this date, there does not exist a general-purpose ML-CFD package which provides 1) powerful state-of-the-art numerical methods, 2) seamless hybridization of ML with CFD, and 3) automatic differentiation (AD) capabilities. AD in particular is essential to ML-CFD research as it provides gradient information and enables optimization of preexisting and novel CFD models. In this work, we propose JAX-FLUIDS: a comprehensive fully-differentiable CFD Python solver for compressible two-phase flows. JAX-FLUIDS allows the simulation of complex fluid dynamics with phenomena like three-dimensional turbulence, compressibility effects, and two-phase flows. Written entirely in JAX, it is straightforward to include existing ML models into the proposed framework. Furthermore, JAX-FLUIDS enables end-to-end optimization. I.e., ML models can be optimized with gradients that are backpropagated through the entire CFD algorithm, and therefore contain not only information of the underlying PDE but also of the applied numerical methods. We believe that a Python package like JAX-FLUIDS is crucial to facilitate research at the intersection of ML and CFD and may pave the way for an era of differentiable fluid dynamics.},
	urldate = {2022-12-02},
	journal = {Computer Physics Communications},
	author = {Bezgin, Deniz A. and Buhendwa, Aaron B. and Adams, Nikolaus A.},
	month = jan,
	year = {2023},
	note = {arXiv:2203.13760 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Fluid Dynamics},
	pages = {108527},
	annote = {Comment: 53 pages, 23 figures},
	file = {arXiv Fulltext PDF:/home/wglao/Zotero/storage/N726GEMB/Bezgin et al. - 2023 - JAX-FLUIDS A fully-differentiable high-order comp.pdf:application/pdf;arXiv.org Snapshot:/home/wglao/Zotero/storage/JXSHIV7P/2203.html:text/html},
}

@article{nguyen22,
	title = {A {Model}-{Constrained} {Tangent} {Slope} {Learning} {Approach} for {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2208.04995},
	doi = {10.48550/arXiv.2208.04995},
	abstract = {Real-time accurate solutions of large-scale complex dynamical systems are in critical need for control, optimization, uncertainty quantification, and decision-making in practical engineering and science applications, especially digital twin applications. This paper contributes in this direction a model-constrained tangent slope learning (mcTangent) approach. At the heart of mcTangent is the synergy of several desirable strategies: i) a tangent slope learning to take advantage of the neural network speed and the time-accurate nature of the method of lines; ii) a model-constrained approach to encode the neural network tangent slope with the underlying governing equations; iii) sequential learning strategies to promote long-time stability and accuracy; and iv) data randomization approach to implicitly enforce the smoothness of the neural network tangent slope and its likeliness to the truth tangent slope up second order derivatives in order to further enhance the stability and accuracy of mcTangent solutions. Rigorous results are provided to analyze and justify the proposed approach. Several numerical results for the transport equation, viscous Burgers equation, and Navier-Stokes equation are presented to study and demonstrate the robustness and long-time accuracy of the proposed mcTangent learning approach.},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Nguyen, Hai V. and Bui-Thanh, Tan},
	month = nov,
	year = {2022},
	note = {arXiv:2208.04995 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Fluid Dynamics, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\wesle\\Zotero\\storage\\RRL3DYYE\\Nguyen and Bui-Thanh - 2022 - A Model-Constrained Tangent Slope Learning Approac.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wesle\\Zotero\\storage\\Z3S4QB5F\\2208.html:text/html},
}

@inproceedings{haeffele17,
	title = {Global {Optimality} in {Neural} {Network} {Training}},
	doi = {10.1109/CVPR.2017.467},
	abstract = {The past few years have seen a dramatic increase in the performance of recognition systems thanks to the introduction of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key issue is that the neural network training problem is nonconvex, hence optimization algorithms may not return a global minima. This paper provides sufficient conditions to guarantee that local minima are globally optimal and that a local descent strategy can reach a global minima from any initialization. Our conditions require both the network output and the regularization to be positively homogeneous functions of the network parameters, with the regularization being designed to control the network size. Our results apply to networks with one hidden layer, where size is measured by the number of neurons in the hidden layer, and multiple deep subnetworks connected in parallel, where size is measured by the number of subnetworks.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Haeffele, Benjamin D. and Vidal, Ren√©},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Algorithm design and analysis, Biological neural networks, Loss measurement, Minimization, Neurons, Optimization, Training},
	pages = {4390--4398},
	file = {Haeffele and Vidal - 2017 - Global Optimality in Neural Network Training.pdf:C\:\\Users\\wesle\\Zotero\\storage\\T4RXE9AF\\Haeffele and Vidal - 2017 - Global Optimality in Neural Network Training.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\wesle\\Zotero\\storage\\5HJ76AKZ\\8099950.html:text/html},
}

@misc{bui22,
	title = {A {Unified} and {Constructive} {Framework} for the {Universality} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2112.14877},
	doi = {10.48550/arXiv.2112.14877},
	abstract = {One of the reasons why many neural networks are capable of replicating complicated tasks or functions is their universal property. Though the past few decades have seen tremendous advances in theories of neural networks, a single constructive framework for neural network universality remains unavailable. This paper is an effort to provide a unified and constructive framework for the universality of a large class of activations including most of existing ones. At the heart of the framework is the concept of neural network approximate identity (nAI). The main result is: \{{\textbackslash}em any nAI activation function is universal\}. It turns out that most of existing activations are nAI, and thus universal in the space of continuous functions on compacta. The framework has the following main properties. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first unified attempt that is valid for most of existing activations. Third, as a by product, the framework provides the first university proof for some of the existing activation functions including Mish, SiLU, ELU, GELU, and etc. Fourth, it provides new proofs for most activation functions. Fifth, it discovers new activations with guaranteed universality property. Sixth, for a given activation and error tolerance, the framework provides precisely the architecture of the corresponding one-hidden neural network with predetermined number of neurons, and the values of weights/biases. Seventh, the framework allows us to abstractly present the first universal approximation with favorable non-asymptotic rate.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Bui-Thanh, Tan},
	month = jan,
	year = {2022},
	note = {arXiv:2112.14877 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	annote = {Comment: fix typos errors, remove results that are not necessary, and add section 9 on non-asymptotic results. Add figures to demonstrate the theoretical results},
	file = {arXiv Fulltext PDF:C\:\\Users\\wesle\\Zotero\\storage\\LE7J3YJF\\Bui-Thanh - 2022 - A Unified and Constructive Framework for the Unive.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wesle\\Zotero\\storage\\PJCLPXAZ\\2112.html:text/html},
}